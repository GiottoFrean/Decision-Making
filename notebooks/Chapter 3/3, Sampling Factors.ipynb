{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "rural-footage",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.insert(0,'../../modules')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01a92bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import factors\n",
    "import exact_inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d153fee",
   "metadata": {},
   "source": [
    "# Sampling top down\n",
    "When sampling a pgm one option is to make the full joint and sample from that. This requires making one large factor which might not be viable. Another option is to sample the top (root) nodes first, then condition all further samples on their parent nodes. This means you can sample the full joint without needing to merge all factors. <br>\n",
    "Example: Making the graph $A$ and $B$ (independent) cause $C$ which then causes $D$ and $E$ (independent given $C$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4bec9683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A  Values (10 dp)\n",
      "0  0.6101553093\n",
      "1  0.3898446907\n",
      "\n",
      "B  Values (10 dp)\n",
      "0  0.853821384\n",
      "1  0.146178616\n",
      "\n",
      "C  A  B  Values (10 dp)\n",
      "0  0  0  0.661781136\n",
      "0  0  1  0.4906691389\n",
      "0  1  0  0.561660912\n",
      "0  1  1  0.742833305\n",
      "1  0  0  0.338218864\n",
      "1  0  1  0.5093308611\n",
      "1  1  0  0.438339088\n",
      "1  1  1  0.257166695\n",
      "\n",
      "D  C  Values (10 dp)\n",
      "0  0  0.2071755147\n",
      "0  1  0.62738997\n",
      "1  0  0.7928244853\n",
      "1  1  0.37261003\n",
      "\n",
      "E  C  Values (10 dp)\n",
      "0  0  0.7921225093\n",
      "0  1  0.958033264\n",
      "1  0  0.2078774907\n",
      "1  1  0.041966736\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# making random values\n",
    "A = factors.Factor([\"A\"],[2])\n",
    "B = factors.Factor([\"B\"],[2])\n",
    "C = factors.Factor([\"C\",\"A\",\"B\"],[2,2,2])\n",
    "D = factors.Factor([\"D\",\"C\"],[2,2])\n",
    "E = factors.Factor([\"E\",\"C\"],[2,2])\n",
    "all_factors = []\n",
    "for f in [A,B,C,D,E]:\n",
    "    f.set_all(np.random.rand(np.prod(f.array.shape)))\n",
    "    cond_f = factors.condition(f,list(f.names[1:]))\n",
    "    all_factors.append(cond_f)\n",
    "for f in all_factors:\n",
    "    print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17115b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Samples by setting the variables in order from top to bottom. Requires a directed factor graph. This is probably not going to work for an undirected graph.\n",
    "def joint_sample_top_down(all_factors):\n",
    "    assigned_variable_names = []\n",
    "    variable_assignments = []\n",
    "    remaining_factors = all_factors.copy()\n",
    "    \n",
    "    while(len(remaining_factors)>0): #  while variables haven't been assigned\n",
    "        new_assigned_variable_names = assigned_variable_names.copy()\n",
    "        new_variable_assignments = variable_assignments.copy()\n",
    "        new_remaining_factors = []\n",
    "        for f in remaining_factors:\n",
    "            # grab variables with just themselves in the factor, and every variable which has all parent values set.\n",
    "            if(len(f.names)==1 or np.prod([i in assigned_variable_names for i in f.names[1:]])==1):\n",
    "                conditioned_factor = factors.drop_variables(f,assigned_variable_names,variable_assignments)\n",
    "                new_variable_assignments.append(factors.sample(conditioned_factor,1)[0][0]) # sample a new value\n",
    "                new_assigned_variable_names.append(f.names[0])\n",
    "            else:\n",
    "                new_remaining_factors.append(f)\n",
    "        assigned_variable_names = new_assigned_variable_names\n",
    "        variable_assignments = new_variable_assignments\n",
    "        remaining_factors = new_remaining_factors\n",
    "    return assigned_variable_names,variable_assignments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b957d8",
   "metadata": {},
   "source": [
    "**Checking the results:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d66f328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C  Values (10 dp)\n",
      "0  0.6244\n",
      "1  0.3756\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_samples = []\n",
    "for n in range(5000):\n",
    "    names,assignments = joint_sample_top_down(all_factors)\n",
    "    all_samples.append(assignments)\n",
    "all_samples = np.array(all_samples)\n",
    "prob_C_sampled = factors.Factor([\"C\"],[2])\n",
    "prob_C_sampled.set([0],np.sum(all_samples[:,2]==0)/all_samples.shape[0])\n",
    "prob_C_sampled.set([1],np.sum(all_samples[:,2]==1)/all_samples.shape[0])\n",
    "print(prob_C_sampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dbf13cdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C  Values (10 dp)\n",
      "0  0.6178124988\n",
      "1  0.3821875012\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prob_C_exact = exact_inference.sum_product_variable_elimination(all_factors,[],[],[\"A\",\"B\",\"D\",\"E\"])\n",
    "print(prob_C_exact)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85da6d04",
   "metadata": {},
   "source": [
    "# Direct Sampling, Likelihood weighted sampling, Gibbs Sampling\n",
    "One problem with the above is that you can only sample the full joint this way. But often we need to sample a conditional distribution instead. If it is difficult to sample if the observed variables are the bottom nodes in a pgm, as it means sampling all parent nodes backwards. Whereas if the root nodes are known then it is easy, as above. There are a few options to deal with this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd470bb",
   "metadata": {},
   "source": [
    "### Direct Sampling\n",
    "With direct sampling you sample the full joint like above and simply throw away all samples which don't match the observed variables. This is basically rejection sampling. Say we want to know $P(C)$ as above, but only if $A$ is 0 (using the same samples as before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e5f4cb92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C  Values (10 dp)\n",
      "0  0.6388979993\n",
      "1  0.3611020007\n",
      "\n"
     ]
    }
   ],
   "source": [
    "samples_where_A0 = all_samples[all_samples[:,0]==0]\n",
    "prob_C_given_A_sampled = factors.Factor([\"C\"],[2])\n",
    "prob_C_given_A_sampled.set([0],np.sum(samples_where_A0[:,2]==0)/samples_where_A0.shape[0])\n",
    "prob_C_given_A_sampled.set([1],np.sum(samples_where_A0[:,2]==1)/samples_where_A0.shape[0])\n",
    "print(prob_C_given_A_sampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e41774c",
   "metadata": {},
   "source": [
    "**checking the approximation is close to the exact value**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f71ea0d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C  Values (10 dp)\n",
      "0  0.6367682211\n",
      "1  0.3632317789\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prob_C_given_A_exact = exact_inference.sum_product_variable_elimination(all_factors,[\"A\"],[0],[\"B\",\"D\",\"E\"])\n",
    "print(prob_C_given_A_exact)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ded5825",
   "metadata": {},
   "source": [
    "### Likelihood weighting\n",
    "With Likelihood weighting you sample all unknown variables as normal, but set the known variables to their value. This means all children are correctly sampled based on the parent values, but the parents are not sampled based on the children. So, you are sampling correct looking variables, but with a slightly incorect distribution. This is Importance Sampling applied to factors.\n",
    "$$P(X=x_n) = \\int \\mathbb{1}(x=x_n)p(x) dx \\approx \\frac{1}{N} \\sum_{i=1}^N \\mathbb{1}(x_i=x_n) $$\n",
    "Becomes:\n",
    "$$\\int \\mathbb{1}(x=x_n)p(x) dx \\approx \\frac{1}{\\sum \\frac{p(x_n)}{q(x_n)}} \\sum_{i=1}^N \\frac{\\mathbb{1}(x_i=x_n)p(x_n)}{q(x_n)} $$\n",
    "Using the formula for normalized importance sampling. The distribution we sample from, $q(x)$, is constructed as above by setting observed values. The true probability $p(x)$ is the multiplication of all of the normalized conditional probabilities regardless of whether the observed value was set or not. $q(x)$ is the same, but $1$ whereever the value was assigned by evidence. Therefore, $\\frac{p(x)}{q(x)}$ (the weight) is just the product of the probabilities at the observed variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6cfb1138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Likelihood weighted sampling. For normalized importance sampling to pgms. \n",
    "# Similar to above joint method but sets all observed variables and returns weight.\n",
    "def likelihood_weighting_top_down(all_factors,known_vars,evidence):\n",
    "    assigned_variable_names = []\n",
    "    variable_assignments = []\n",
    "    weight = 1\n",
    "    remaining_factors = all_factors.copy()\n",
    "    \n",
    "    while(len(remaining_factors)>0):\n",
    "        new_assigned_variable_names = assigned_variable_names.copy()\n",
    "        new_variable_assignments = variable_assignments.copy()\n",
    "        new_remaining_factors = []\n",
    "        for f in remaining_factors:\n",
    "            if(len(f.names)==1 or np.prod([i in assigned_variable_names for i in f.names[1:]])==1):\n",
    "                var_dropped_factor = factors.drop_variables(f,assigned_variable_names,variable_assignments)\n",
    "                conditioned_factor = factors.condition(var_dropped_factor)\n",
    "                if(f.names[0] in known_vars):\n",
    "                    evid = evidence[known_vars.index(f.names[0])]\n",
    "                    new_variable_assignments.append(evid)\n",
    "                    weight *= conditioned_factor.get([evid])\n",
    "                else:\n",
    "                    sample = factors.sample(conditioned_factor,1)[0][0]\n",
    "                    new_variable_assignments.append(sample)\n",
    "                    \n",
    "                new_assigned_variable_names.append(f.names[0])\n",
    "            else:\n",
    "                new_remaining_factors.append(f)\n",
    "        assigned_variable_names = new_assigned_variable_names\n",
    "        variable_assignments = new_variable_assignments\n",
    "        remaining_factors = new_remaining_factors\n",
    "    return assigned_variable_names,variable_assignments,weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd02764",
   "metadata": {},
   "source": [
    "**Checking results again:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7f0e861b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C  Values (10 dp)\n",
      "0  0.639\n",
      "1  0.361\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_LW_samples = []\n",
    "all_LW_weights = []\n",
    "for n in range(1000):\n",
    "    names,assignments,weight = likelihood_weighting_top_down(all_factors,[\"A\"],[0])\n",
    "    all_LW_samples.append(assignments)\n",
    "    all_LW_weights.append(weight)\n",
    "all_LW_samples = np.array(all_LW_samples)\n",
    "all_LW_weights = np.array(all_LW_weights)\n",
    "\n",
    "prob_C_given_A_LW_sampled = factors.Factor([\"C\"],[2])\n",
    "prob_C_given_A_LW_sampled.set([0],np.sum(all_LW_weights*(all_LW_samples[:,2]==0))/np.sum(all_LW_weights))\n",
    "prob_C_given_A_LW_sampled.set([1],np.sum(all_LW_weights*(all_LW_samples[:,2]==1))/np.sum(all_LW_weights))\n",
    "print(prob_C_given_A_LW_sampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73f2bb8",
   "metadata": {},
   "source": [
    "### Gibbs sampling\n",
    "An alternative was to do inference is to generate samples of the conditional distribution using gibbs sampling. Gibbs starts with a random set of variable values and proceedes by conditioning each variable on all variables within its markov blanket. This makes it tractable. It is a variant of the Markov Chain Monte Carlo (MCMC) technique for generating samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f597c8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gibbs sampling. Done with a step method and a method to generate lots of samples. \n",
    "# Quite complicated, but mainly because of indexing issues. I pass the markov blanket factors so they don't need to be recalculated.\n",
    "# The markov blankets are just the factor product of all variables connected to a node. The gibbs step updates a sample in place.\n",
    "def gibbs_step(all_variable_markov_blankets,fixed_variables,all_variable_names,current_state_values):\n",
    "    for var_name in all_variable_names:\n",
    "        if(not var_name in fixed_variables):\n",
    "            joint = all_variable_markov_blankets[all_variable_names.index(var_name)]\n",
    "            index = all_variable_names.index(var_name)\n",
    "            other_var_names = list(all_variable_names[:index])+list(all_variable_names[(index+1):])\n",
    "            other_var_vals = list(current_state_values[:index])+list(current_state_values[(index+1):])\n",
    "            slc = [slice(None)]*len(joint.names)\n",
    "            for i in range(len(all_variable_names)):\n",
    "                if(all_variable_names[i] in joint.names):\n",
    "                    slc[joint.names.index(all_variable_names[i])]=slice(current_state_values[i],current_state_values[i]+1)\n",
    "            joint_index = joint.names.index(var_name)\n",
    "            slc[joint_index]=slice(0,joint.array.shape[joint_index])\n",
    "            array_slice = np.squeeze(joint.array[tuple(slc)])\n",
    "            norm_array_slice = array_slice/np.sum(array_slice)\n",
    "            cond_joint = factors.drop_variables(joint,other_var_names,other_var_vals)\n",
    "            sample = np.random.choice(np.arange(joint.array.shape[joint_index]),1,p=norm_array_slice)\n",
    "            #print(sample)\n",
    "            current_state_values[index]=sample\n",
    "    return current_state_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8dbdd950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gibbs sampling begins by making a random vector of values and then applies the gibbs step repeatedly. \n",
    "def gibbs_sampling(all_factors,known_vars,evidence,N):\n",
    "    all_names = []\n",
    "    # All this below is just to make a general random vector for a factor (and set the evidence)\n",
    "    for f in all_factors:\n",
    "        all_names+=list(f.names)\n",
    "    all_names = list(np.unique(all_names))\n",
    "    current_state = np.zeros(len(all_names)).astype(int) # start with 0's\n",
    "    for i,name in enumerate(all_names):\n",
    "        for f in all_factors: # find a factor to get the sample\n",
    "            if(name in f.names):\n",
    "                shape = f.array.shape[list(f.names).index(name)]\n",
    "                current_state[i]=np.random.randint(0,shape)\n",
    "                break\n",
    "    for i in range(len(known_vars)):\n",
    "        current_state[all_names.index(known_vars[i])]=evidence[i]\n",
    "    \n",
    "    # This is for making the markov blanket factors\n",
    "    all_variable_markov_blankets = []\n",
    "    for var_name in all_names:\n",
    "        markov_blanket = []\n",
    "        for f in all_factors:\n",
    "            if(var_name in f.names):\n",
    "                markov_blanket.append(f)\n",
    "        all_variable_markov_blankets.append(factors.multiple_factor_product(markov_blanket))\n",
    "    \n",
    "    # This is the core loop\n",
    "    all_visited_states = []\n",
    "    for n in range(N):\n",
    "        current_state = gibbs_step(all_variable_markov_blankets,known_vars,all_names,current_state)\n",
    "        all_visited_states.append(current_state.copy())\n",
    "    return np.array(all_visited_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "93926dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = gibbs_sampling(all_factors,[\"A\"],[0],10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3dd84a81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6161616161616161\n"
     ]
    }
   ],
   "source": [
    "average_C_given_A = np.mean(samples[100::10]==0,axis=0)[2]\n",
    "print(average_C_given_A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df547deb",
   "metadata": {},
   "source": [
    "As expected, this value matches the exact value closely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "77c578eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C  Values (10 dp)\n",
      "0  0.6367682211\n",
      "1  0.3632317789\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(exact_inference.sum_product_variable_elimination(all_factors,[\"A\"],[0],[\"B\",\"D\",\"E\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd3588b",
   "metadata": {},
   "source": [
    "The main sections of code here can be found in the pgm_sampling module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "11758763",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pgm_sampling #gibbs_sampling,likelihood_weighting_top_down,joint_sample_top_down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "da1838fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 1 1 0]\n",
      " [0 0 1 1 0]\n",
      " [1 0 1 1 0]\n",
      " [0 0 1 1 0]\n",
      " [0 0 1 1 0]\n",
      " [1 0 1 0 0]\n",
      " [1 1 1 0 0]\n",
      " [0 0 1 0 0]\n",
      " [0 0 1 1 0]\n",
      " [0 0 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "gibbs_samples = pgm_sampling.gibbs_sampling(all_factors,[\"C\"],[1],10)\n",
    "print(gibbs_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0cc04123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "_,likelihood_sample,weight = pgm_sampling.likelihood_weighting_top_down(all_factors,[\"C\"],[1])\n",
    "print(likelihood_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "73d974a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 0, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "_,joint_sample = pgm_sampling.joint_sample_top_down(all_factors)\n",
    "print(joint_sample)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
