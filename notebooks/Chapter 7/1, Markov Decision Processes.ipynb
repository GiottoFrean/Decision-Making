{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72bae20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.insert(0,'../../modules')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0a447708",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaab6437",
   "metadata": {},
   "source": [
    "# Markov Decision Processes\n",
    "Many problems require making a set of sequential decisions (e.g chess, go, card games). <br>\n",
    "One of the approaches to dealing with sequential problems is to assume a markov condition. In a markov decision process(MDP) we have a set of actions $A$ and a set of states $S$. Iteratively at each timestep we make an action which transitions the current state $S_t$ to a new state $S_{t+1}$ with a probability $P(S_{t+1}|S_t,A_t)$. At each timestep a reward value $R_t$ is given based on the current state and action with probability $P(R_t|S_t,A_t)$. The markov condition is that the next state only depends on the current state and current action."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc73ef24",
   "metadata": {},
   "source": [
    "When we make a finite number of decisions the utility (which we want to maximize) is given as the sum of rewards at all timesteps, $\\sum_{t=1}^n R_t$. With an infinite set of steps this becomes infinite, so often a discount factor is included which grows with time, $\\sum_{t=1}^\\infty \\lambda^{t-1} R_t$. Alternatively we can use an average instead, but that can be computationally difficult. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf297593",
   "metadata": {},
   "source": [
    "Because we don't know the reward at each given time step we need to take an expectation over all possible rewards so the above becomes $\\sum_{t=1}^\\infty \\lambda^{t-1} \\sum_i P(R_t^i)R_t^i$. Because of the markov condition we know the probability of $R_t$ given $S_t$ and $A_t$ and can rewrite this as:\n",
    "$$\\sum_{t=1}^\\infty \\lambda^{t-1} \\sum_i P(R_t^i|S_t,A_t)R_t^i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d719e12f",
   "metadata": {},
   "source": [
    "While there are non-stationary MDPs it is useful to assume $P(S_{t+1}|S_t,A_t)$ and $P(R_t|S_t,A_t)$ are the same for all $t$. Stationary MDPs can have the transition from one state to another determined by a function $T(s'|s,a)$ which doesn't depend on $t$. The expectation over the reward given the current state $s$ and executing action function $a$ is $R(s,a)$. So the above is:\n",
    "$$\\sum_{t=1}^\\infty \\lambda^{t-1} R(S_t,A_t)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2f18bb",
   "metadata": {},
   "source": [
    "In a MDP problem we are trying to find a good policy, which tells us which action to take given previous actions and the current state. With an infinite time stationary MDP we get policies not depending on $t$. We call the stationary policy $\\pi$, which maps state to action $\\pi(s)$. We also have a stationary transition function $T(s'|s,a)$ which is the probability of moving from a state and action to a new state."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1095d2ab",
   "metadata": {},
   "source": [
    "With this in mind the above overall utility function can be written:\n",
    "$$\\sum_{t=1}^\\infty \\lambda^{t-1} R(S_t,\\pi(S_t))$$\n",
    "Let $U_k^\\pi(s)=\\sum_{t=1}^k \\lambda^{t-1} R(S_t,\\pi(S_t))$ with starting state $S_1$ (counting $t$ from 1 **relative** to $s$, so $S_1$=$s$). <br>\n",
    "So, $U_1^\\pi(s)=R(s,\\pi(s))$<br>\n",
    "Then $U_\\infty^\\pi(s)$ is the value we want to maximize. <br>\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    U_{k+1}^\\pi(s)&=\\sum_{t=1}^{k+1} \\lambda^{t-1} R(S_t,\\pi(S_t)) \\\\\n",
    "    &= R(s,\\pi(s)) + \\lambda \\sum_{t=2}^{k+1} \\lambda^{t-1} R(S_t,\\pi(S_t)) \\\\\n",
    "    &= R(s,\\pi(s)) + \\lambda U_k^\\pi(s')\n",
    "\\end{aligned}\n",
    "$$\n",
    "Where $s'$ is the state that follows $s$. However, as we don't know the actual value of $s'$ we use the expectation:\n",
    "$$U_{k+1}^\\pi(s)=R(s,\\pi(s)) + \\lambda \\sum_{s'} T(s'|s,\\pi(s)) U_k^\\pi(s')$$\n",
    "This formula is very intuitive. The value of a state ($U^\\pi(s)$) is the immediate reward plus the expected value of the probable future states."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef202d34",
   "metadata": {},
   "source": [
    "For an infinite horizon the solution for $U^\\pi$ can be found iteratively:\n",
    "$$U^\\pi=R(s,\\pi(s))+\\lambda\\sum_{s'}T(s'|s,\\pi(s))U^\\pi(s')$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74e7687",
   "metadata": {},
   "source": [
    "### Evaluating a simple decision:\n",
    "**W** is a wall <br>\n",
    "**F** is a fire <br>\n",
    "**G** is gold <br>\n",
    "**S** is empty <br>\n",
    "**Y** is you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3090d1fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAMMAAADnCAYAAACjQuKKAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAADEUlEQVR4nO3dQWrbUBRAUat0S513Z5F31nkXpU4uGcXfxaor++sc8CTGkSBcHjwp1rJt2wW4XL4dfQLwKsQAEQNEDBAxQL6P3rxer1ZNTOfj42P56ucmA0QMEDFAxAARA0QMEDFAxAARA0QMEDFAxAARA0QMEDFAxAARA0QMEDFAxAARA0QMEDFAxAARA0QMEDFAxAARA0QMEDFAxAARA0QMEDFAhs90ezXruh59ClPbBk/wW758CtrfeZe/m8kAEQNEDBAxQMQAEQNEDJC3us7APqPrCHs+u+caxCsxGSBigIgBIgaIGCBigIhhMtt2+3XEMZ953H9NDBAxQMQAEQNEDBAxQMQwmWW5/TrimO90R6sYIGKAiAEiBogYIGKA+EKAE7m35jzDP/2PmAwQMUDEABEDRAwQMUDEAHGdgU9nuJYwYjJAxAARA0QMEDFAxACxWj3Ive/jPfmW8xAmA0QMEDFAxAARA0QMEKvVJ9rzOLPRZ61dn8NkgIgBIgaIGCBigIgBYrW605716TOOae36OJMBIgaIGCBigIgBIgaIGCBigIgBIgaIGCBigIgBIgbIaW7h3n79vvne8vPHw7/XLdPzMBkgYoCIASIGiBggYoBMtVodrU/3fG7P6vXmMe+8b2X7/5kMEDFAxAARA0QMEDFAxACZ6jrD6HrAs27hHvGAw/diMkDEABEDRAwQMUDEAJlqtXrELdwecDgPkwEiBogYIGKAiAEiBshUq9WRR+9o5TxMBogYIGKAiAEiBogYIFOtVh/9x37PdONyMRngkxggYoCIASIGiBggYoCIASIGiBggYoCIASIGiBggYoCIASIGiBggYoCIASIGiBggYoCIASIGiBggYoCIASIGiBggYoCIASIGiBggYoCIAfJWDzhc1/XoU2BiJgNEDBAxQMQAEQNEDBAxQMQAEQNEDBAxQMQAEQNEDBAxQMQAEQNEDBAxQMQAEQNEDBAxQMQAEQNEDBAxQMQAEQNEDBAxQMQAEQNEDBAxQJZt244+B3gJJgNEDBAxQMQAEQNEDJA/I+dV25jhGJIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "world = np.array([['W','W','W','W','W'],\n",
    "                  ['W','S','S','G','W'],\n",
    "                  ['W','S','F','S','W'],\n",
    "                  ['W','Y','F','S','W'],\n",
    "                  ['W','S','S','S','W'],\n",
    "                  ['W','W','W','W','W']])\n",
    "\n",
    "\n",
    "def display_world(world):\n",
    "    wall_tile = np.ones((5,5))\n",
    "    empty_tile = np.zeros((5,5))\n",
    "    you_tile = np.array([[0,0,1,0,0],\n",
    "                         [0,1,1,1,0],\n",
    "                         [0,0,1,0,0],\n",
    "                         [0,1,1,1,0],\n",
    "                         [0,1,0,1,0]])*2\n",
    "    gold_tile = np.array([[0,0,1,0,0],\n",
    "                          [0,1,1,1,0],\n",
    "                          [1,1,1,1,1],\n",
    "                          [0,1,1,1,0],\n",
    "                          [0,0,1,0,0]])*3\n",
    "    fire_tile = np.array([[0,0,0,0,0],\n",
    "                          [0,0,1,0,0],\n",
    "                          [0,1,1,1,0],\n",
    "                          [1,1,1,1,1],\n",
    "                          [1,1,1,1,1]])*4\n",
    "    tile_dict = dict(zip(['W','S','Y','G','F'],[wall_tile,empty_tile,you_tile,gold_tile,fire_tile]))\n",
    "\n",
    "    image = np.zeros((world.shape[0]*5,world.shape[1]*5))\n",
    "    for row in range(world.shape[0]):\n",
    "        for col in range(world.shape[1]):\n",
    "            tile = tile_dict[world[row,col]]\n",
    "            image[row*5:(row+1)*5,col*5:(col+1)*5]=tile\n",
    "    colormap =  colors.ListedColormap(['black','grey','pink','yellow','red'])\n",
    "    plt.imshow(image,cmap=colormap)\n",
    "    plt.axis(False)\n",
    "    plt.show()\n",
    "            \n",
    "display_world(world)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa5f49b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
