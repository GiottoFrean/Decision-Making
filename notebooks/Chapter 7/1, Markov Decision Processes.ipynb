{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72bae20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.insert(0,'../../modules')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaab6437",
   "metadata": {},
   "source": [
    "# Markov Decision Processes\n",
    "Many problems require making a set of sequential decisions (e.g chess, go, card games). <br>\n",
    "One of the approaches to dealing with sequential problems is to assume a markov condition. In a markov decision process(MDP) we have a set of actions $A$ and a set of states $S$. Iteratively at each timestep we make an action which transitions the current state $S_t$ to a new state $S_{t+1}$ with a probability $P(S_{t+1}|S_t,A_t)$. At each timestep a reward value $R_t$ is given based on the current state and action with probability $P(R_t|S_t,A_t)$. The markov condition is that the next state only depends on the current state and current action."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc73ef24",
   "metadata": {},
   "source": [
    "When we make a finite number of decisions the utility (which we want to maximize) is given as the sum of rewards at all timesteps, $\\sum_{t=1}^n R_t$. With an infinite set of steps this becomes infinite, so often a discount factor is included which grows with time, $\\sum_{t=1}^\\infty \\lambda^{t-1} R_t$. Alternatively we can use an average instead, but that can be computationally difficult. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf297593",
   "metadata": {},
   "source": [
    "Because we don't know the reward at each given time step we need to take an expectation over all possible rewards so the above becomes $\\sum_{t=1}^\\infty \\lambda^{t-1} \\sum_i P(R_t^i)R_t^i$. Because of the markov condition we know the probability of $R_t$ given $S_t$ and $A_t$ and can rewrite this as:\n",
    "$$\\sum_{t=1}^\\infty \\lambda^{t-1} \\sum_i P(R_t^i|S_t,A_t)R_t^i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d719e12f",
   "metadata": {},
   "source": [
    "While there are non-stationary MDPs it is useful to assume $P(S_{t+1}|S_t,A_t)$ and $P(R_t|S_t,A_t)$ are the same for all $t$. Stationary MDPs can have the transition from one state to another determined by a function $T(s'|s,a)$ which doesn't depend on $t$. The expectation over the reward given the current state $s$ and executing action function $a$ is $R(s,a)$. So the above is:\n",
    "$$\\sum_{t=1}^\\infty \\lambda^{t-1} R(S_t,A_t)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2f18bb",
   "metadata": {},
   "source": [
    "In a MDP problem we are trying to find a good policy, which tells us which action to take given previous actions and the current state. With an infinite time stationary MDP we get policies not depending on $t$. We call the stationary policy $\\pi$, which maps state to action $\\pi(s)$. We also have a stationary transition function $T(s'|s,a)$ which is the probability of moving from a state and action to a new state."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1095d2ab",
   "metadata": {},
   "source": [
    "With this in mind the above overall utility function can be written:\n",
    "$$\\sum_{t=1}^\\infty \\lambda^{t-1} R(S_t,\\pi(S_t))$$\n",
    "Let $U_k^\\pi(s)=\\sum_{t=1}^k \\lambda^{t-1} R(S_t,\\pi(S_t))$ with starting state $s$. <br>\n",
    "So, $U_1^\\pi(s)=R(s,\\pi(s))$<br>\n",
    "Then $U_\\infty^\\pi(s)$ is the value we want to maximize. <br>\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    U_{k+1}^\\pi(s)&=\\sum_{t=1}^{k+1} \\lambda^{t-1} R(S_t,\\pi(S_t)) \\\\\n",
    "    &= R(s,\\pi(s)) + \\lambda \\sum_{t=2}^{k+1} \\lambda^{t-1} R(S_t,\\pi(S_t)) \\\\\n",
    "    &= R(s,\\pi(s)) + \\lambda U_k^\\pi(s')\n",
    "\\end{aligned}\n",
    "$$\n",
    "Where $s'$ is the state that follows $s$. However, as we don't know the actual value of $s'$ we use the expectation:\n",
    "$$U_{k+1}^\\pi(s)=R(s,\\pi(s)) + \\lambda \\sum_{s'} T(s'|s,\\pi(s)) U_k^\\pi(s')$$\n",
    "This formula is very intuitive. The value of a state ($U^\\pi(s)$) is the immediate reward plus the expected reward of the probable future states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee09c9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
