{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99b17ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.insert(0,'../../modules')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01303dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import common_plots\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027c8440",
   "metadata": {},
   "source": [
    "# The Expectation-Maximization algorithm\n",
    "The Em algorithm is used to improve the estimate of model parameters $\\theta$. There are two steps to this. Firstly, a distribution is found over all unknown/missing values using an inference algorithm. Then secondly, the model parameters are maximized using that distribution. The origins come from the Evidence Lower Bound (ELBO):\n",
    "### ELBO EM algorithm derivation:\n",
    "The Kullback-Leibler divergence is a measure of the difference between two distributions $P$ and $Q$: $$KL(Q||P)=\\sum_x Q(x)\\log \\frac{Q(x)}{P(x)}$$\n",
    "The divergence can be thought as the average difference in numbers of  bits required to encode samples from distribution $P$ using a code made with distribution $Q$ instead. Say we have model parameters $\\theta$, observed data $X$ and unobserved data $Z$. Define $P(Z|X,\\theta)$ as the true distribution of $Z$ given $X$ and $\\theta$ and $Q(Z)$ as an arbitrary distribution over $Z$. If look at the KL divergence between the two we get: \n",
    "$$\n",
    "\\begin{aligned}\n",
    "    KL(Q(Z)||P(Z|X,\\theta))&=\\sum_z Q(z)\\log \\bigg(\\frac{Q(z)}{P(z|X,\\theta)}\\bigg) \\\\\n",
    "    KL(Q(Z)||P(Z|X,\\theta))&=\\sum_z Q(z)\\log \\bigg(\\frac{Q(z)P(X,\\theta)}{P(z,X,\\theta)}\\bigg) \\\\\n",
    "    KL(Q(Z)||P(Z|X,\\theta))&=\\sum_z Q(z)\\log \\bigg(\\frac{Q(z)P(X,\\theta)}{P(z,X|\\theta)p(\\theta)}\\bigg) \\\\\n",
    "    KL(Q(Z)||P(Z|X,\\theta))&=\\sum_z Q(z)\\log \\bigg(\\frac{Q(z)}{P(z,X|\\theta)}\\frac{P(X,\\theta)}{p(\\theta)} \\bigg)\\\\\n",
    "    KL(Q(Z)||P(Z|X,\\theta))&=\\sum_z Q(z)\\bigg(\\log \\frac{Q(z)}{P(z,X|\\theta)} + \\log P(X|\\theta)\\bigg) \\\\\n",
    "    KL(Q(Z)||P(Z|X,\\theta))&=\\bigg(\\sum_z Q(z)\\log \\frac{Q(z)}{P(z,X|\\theta)}\\bigg) + \\log P(X|\\theta) \\\\\n",
    "    KL(Q(Z)||P(Z|X,\\theta))&=-\\bigg(\\sum_z Q(z)\\log \\frac{P(z,X|\\theta)}{Q(z)}\\bigg) + \\log P(X|\\theta) \\\\\n",
    "    \\log P(X|\\theta)&=\\bigg(\\sum_z Q(z)\\log \\frac{P(z,X|\\theta)}{Q(z)}\\bigg)+KL(Q||P(z|X,\\theta)) \\\\\n",
    "\\end{aligned}\n",
    "$$ <br>\n",
    "As the $KL$ divergence is always positive the first term on the right hand side provides a lower bound on $P(X|\\theta)$, hence the name. As $P(X|\\theta)$ is a constant increasing the first term on the right hand side decreases the $KL$ divergence between the two distributions $Q(Z)$ and $P(Z|X,\\theta)$. If we say $Q(Z) = P(Z|X,\\theta)$ this makes the $KL$ divergence 0. The form can be rewritten:\n",
    "$$\\sum_z P(z|X,\\theta)\\log \\frac{P(z,X|\\theta)}{P(z|X,\\theta)}=\\sum_z P(z|X,\\theta)\\log P(z,X|\\theta) - \\sum_z P(z|X,\\theta)\\log P(z|X,\\theta)$$\n",
    "Thus increasing $\\sum_z Q(z)\\log \\frac{P(z,X|\\theta)}{Q(z)}$ by optimizing $\\theta$ will push the data log likelihood up. The EM algorithm works by first infering $P(Z|X,\\theta_\\text{old})$, then finding $\\theta_\\text{new}$ using the infered values. This just becomes a normal MLE problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2cf4566",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c82b3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
