{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8ac0097",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f17d42",
   "metadata": {},
   "source": [
    "# Integrating out the model\n",
    "Much of machine learning concerns generating preditions based on past data. We want to know $P(X_\\text{new}|X_\\text{old})$. <br>\n",
    "But in order to generate good predictions it is necessary to have a model with parameters $\\theta$. The above can be rewritten\n",
    "$$P(X_\\text{new}|X_\\text{old})=\\int P(X_\\text{new},\\theta|X_\\text{old}) d\\theta$$\n",
    "Which can in turn be expanded with the product rule: <br>\n",
    "$$P(X_\\text{new}|X_\\text{old})=\\int P(X_\\text{new}|\\theta,X_\\text{old})P(\\theta|X_\\text{old}) d\\theta$$\n",
    "Making the assumption that the model encodes the previous data this becomes:\n",
    "$$P(X_\\text{new}|X_\\text{old})=\\int P(X_\\text{new}|\\theta)P(\\theta|X_\\text{old}) d\\theta$$\n",
    "This formula represents the purest way to predict data with a parameter based model. The parameters of the model are integrated out over all possible values, where the probability of each prediction is weighed by the model probability. For instance, consider that there are two coins ($C_1$ and $C_2$), one of which is selected and used in a single flip. $C_1$ has heads on both sides and from previous data has a $10\\%$ chance of being selected. $C_2$ is fair and has a $90%$ chance of being selected. The correct way to predict new data is to apply the above formula. Say you want to know the probability of heads ($H$). That is given by the above formula as: $p(H)=1\\times0.1 + 0.5\\times0.9=0.55$. This is easy to verify with sampling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "926b80fb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "estimate 0.5524\n"
     ]
    }
   ],
   "source": [
    "total_heads = 0\n",
    "total = 0\n",
    "for sample in range(10000):\n",
    "    coin = np.random.choice([\"C1\",\"C2\"],p=[0.1,0.9])\n",
    "    if(coin==\"C1\"):\n",
    "        total_heads+=1\n",
    "        total+=1\n",
    "    if(coin==\"C2\"):\n",
    "        if(np.random.rand()>=0.5):\n",
    "            total_heads+=1\n",
    "        total+=1\n",
    "prob_heads_estimates = total_heads/total\n",
    "print(\"estimate\",prob_heads_estimates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54bdfdf",
   "metadata": {},
   "source": [
    "The issue with this approach is it is not tractable in many cases. However, for a binomial probability the model is analytical - defined as the beta-binomial model. And "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4d8639",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
