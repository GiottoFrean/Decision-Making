{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4cd570c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20103682",
   "metadata": {},
   "source": [
    "# Maximum Likelihood Estimate (MLE)\n",
    "One way to make predictions is to use the most likely model parameters which would generate the data rather than integrating out all parameters.\n",
    "$$P(X_\\text{new}|X_\\text{old})=P(X_\\text{new}|\\theta)$$\n",
    "where $$\\theta = \\text{argmax}_\\theta P(X_\\text{old}|\\theta)$$\n",
    "We use $D$ to refer to old data in some cases. <br>\n",
    "Often it is assumed that the data is independent and identically distributed, which means:\n",
    "$$P(D|\\theta)=\\prod_i P(D_i|\\theta)$$\n",
    "Another common practice is to use the log likelihood of the data, as $$\\text{argmax} (x) = \\text{argmax} (\\log(x))$$\n",
    "This turns the above product into a sum:\n",
    "$$P(D|\\theta)\\propto \\sum_i \\log(P(D_i|\\theta))$$\n",
    "This is much more numerically stable as a product of many numbers less than 1 gets very very small. <br>\n",
    "**examples of maximum likelihood estimates:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e5e23f",
   "metadata": {},
   "source": [
    "### Categorical\n",
    "The binomial distribution describes the distribution with only two possible outcomes defined by a single value parameter $\\theta$. Samples are akin to flipping a coin with a certain bentness. The predicted value $k$ is the number of times one outcome occurs in a given number of samples $n$. The number of times the other result happens is just $n-k$. The probability is defined for $\\theta$: <br>\n",
    "$$P(k|n,\\theta)=\\frac{\\theta^k (1-\\theta)^{n-k}n!}{k!(n-k)!}$$\n",
    "We want a distribution over $\\theta$ and are using the maximum likelihood to do this so need to take the $\\text{argmax}(\\theta)$ of the above. Constants and normalizations not depending on $\\theta$ can thus be removed: \n",
    "$$P(k|n,\\theta)\\propto \\theta^k (1-\\theta)^{n-k}$$\n",
    "Maximizing this is the same as maximizing the likelihood, so:\n",
    "$$l(\\theta)\\propto k\\ln(\\theta)+(n-k)\\ln(1-\\theta)$$\n",
    "As the function is convex we can set the gradient to 0 to get the maximum:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "  \\nabla l(\\theta)&=\\frac{k}{\\theta}-\\frac{n-k}{1-\\theta} \\\\\n",
    "  0&=\\frac{k}{\\theta}-\\frac{n-k}{1-\\theta} \\\\\n",
    "  \\frac{n-k}{1-\\theta}&=\\frac{k}{\\theta} \\\\\n",
    "  \\frac{(n-k)\\theta}{(1-\\theta)\\theta}&=\\frac{k(1-\\theta)}{(1-\\theta)\\theta} \\\\\n",
    "  (n-k)\\theta&=k(1-\\theta) \\\\\n",
    "  n\\theta-k\\theta&=k-k\\theta \\\\\n",
    "  n\\theta&=k \\\\\n",
    "  \\theta&=\\frac{k}{n}\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1462bfc3",
   "metadata": {},
   "source": [
    "So the best estimate for the \"bentness\" is just the mean. With enough data this approaches the truth. E.g For a coin:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e314d77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta estimate 0.4934\n"
     ]
    }
   ],
   "source": [
    "total_heads = 0\n",
    "total_flips = 0\n",
    "theta = 0.5\n",
    "for sample in range(10000):\n",
    "    if(np.random.rand()<theta):\n",
    "        total_heads+=1\n",
    "    total_flips+=1\n",
    "prob_heads_estimates = total_heads/total_flips\n",
    "print(\"theta estimate\",prob_heads_estimates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd092af",
   "metadata": {},
   "source": [
    "The same formula also applies for $j$ discrete variables. The maximum likelihood is:\n",
    "$$\\theta_j=\\frac{k_j}{\\sum k}$$\n",
    "Example of a fair dice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2e4b328f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "truth    [0.1667 0.1667 0.1667 0.1667 0.1667 0.1667]\n",
      "estimate [0.1686 0.1613 0.1676 0.1717 0.162  0.1688]\n"
     ]
    }
   ],
   "source": [
    "totals = np.zeros(6)\n",
    "dice_true_probs = np.ones(6)*(1/6)\n",
    "for sample in range(10000):\n",
    "    roll = np.random.choice(np.arange(1,7),p=dice_probs)\n",
    "    totals[roll-1]+=1\n",
    "estimated_probs = totals/np.sum(totals)\n",
    "print(\"truth   \",dice_true_probs.round(4))\n",
    "print(\"estimate\",estimated_probs.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3571830a",
   "metadata": {},
   "source": [
    "### Gaussian\n",
    "The gaussian pdf is:\n",
    "$$ p(x|\\mu,\\sigma^2)=\\mbox{$\\frac{1}{\\sigma\\sqrt{2\\pi}}$}e^{-\\mbox{$\\frac{1}{2} \\frac{(x-\\mu)^2}{\\sigma^2}$}}$$\n",
    "The log is:\n",
    "\n",
    "$$ \\begin{aligned}\n",
    "    p(x|\\mu,\\sigma^2)&=\\ln(\\mbox{$\\frac{1}{\\sigma\\sqrt{2\\pi}}$})-\\frac{1}{2} \\frac{(x-\\mu)^2}{\\sigma^2} \\\\\n",
    "    &=-\\ln(\\sigma\\sqrt{2\\pi})-\\frac{1}{2} \\frac{(x-\\mu)^2}{\\sigma^2} \\\\\n",
    "    &=-\\ln(\\sigma) -\\ln(\\sqrt{2\\pi})-\\frac{1}{2} \\frac{(x-\\mu)^2}{\\sigma^2}\n",
    "\\end{aligned}\n",
    "$$\n",
    "So for $n$ iid (independent identically distributed) data points this becomes the log likelihood:\n",
    "$$-n\\ln(\\sigma) -n\\ln(\\sqrt{2\\pi})-\\sum_{i=1}^n \\frac{1}{2} \\frac{(x_i-\\mu)^2}{\\sigma^2}$$\n",
    "Which is: \n",
    "$$-n\\ln(\\sigma) -n\\ln(\\sqrt{2\\pi})- \\frac{1}{2} \\frac{\\sum_{i=1}^n(x_i-\\mu)^2}{\\sigma^2}$$\n",
    "The constant $-n\\ln(\\sqrt{2\\pi})$ can be dropped when doing MLE <br>\n",
    "This function is also convex, so we can set the gradient to 0 for each variable.\n",
    "$$ \\begin{aligned}\n",
    "    \\nabla\\mu&=\\frac{\\sum_{i=1}^n(x_i-\\mu)}{\\sigma^2} \\\\\n",
    "    0&=\\frac{\\sum_{i=1}^n(x_i-\\mu)}{\\sigma^2} \\\\\n",
    "    0&=\\sum_{i=1}^n(x_i-\\mu) \\\\\n",
    "    \\sum_{i=1}^n \\mu&=\\sum_{i=1}^n x_i \\\\\n",
    "    n\\mu&=\\sum_{i=1}^n x_i \\\\\n",
    "    \\mu&=\\frac{1}{n}\\sum_{i=1}^n x_i \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "So $\\mu$ is just the mean of the samples\n",
    "$$ \\begin{aligned}\n",
    "    \\nabla\\sigma^2&=-\\frac{n}{\\sigma}+\\frac{\\sum_{i=1}^n(x_i-\\mu)^2}{\\sigma^3} \\\\\n",
    "    0&=-\\frac{n}{\\sigma}+\\frac{\\sum_{i=1}^n(x_i-\\mu)^2}{\\sigma^3} \\\\\n",
    "    \\frac{n}{\\sigma}&=\\frac{\\sum_{i=1}^n(x_i-\\mu)^2}{\\sigma^3} \\\\\n",
    "    n&=\\frac{\\sum_{i=1}^n(x_i-\\mu)^2}{\\sigma^2} \\\\\n",
    "    \\sigma^2&=\\frac{\\sum_{i=1}^n(x_i-\\mu)^2}{n}\n",
    "\\end{aligned}\n",
    "$$\n",
    "So, $\\sigma^2$ is just the variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "89e6d312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true mean      3.4053 true sigma      1.4233\n",
      "estimated mean 3.4038 estimated sigma 1.4283\n"
     ]
    }
   ],
   "source": [
    "true_mean = 3.4053\n",
    "true_sigma = 1.4233\n",
    "gaussian_samples = np.random.normal(true_mean,true_sigma,10000)\n",
    "estimated_mean = np.mean(gaussian_samples)\n",
    "estimated_sigma = np.sqrt(np.mean((gaussian_samples-estimated_mean)**2))\n",
    "print(\"true mean     \",true_mean,\"true sigma     \",true_sigma)\n",
    "print(\"estimated mean\",estimated_mean.round(4),\"estimated sigma\",estimated_sigma.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b322866",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
