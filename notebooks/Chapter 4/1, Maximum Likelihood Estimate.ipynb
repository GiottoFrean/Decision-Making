{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4cd570c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.insert(0,'../../modules')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4029d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import factors\n",
    "import factors_sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20103682",
   "metadata": {},
   "source": [
    "# Maximum Likelihood Estimate (MLE)\n",
    "One way to make predictions is to use the most likely model parameters which would generate the data rather than integrating out all parameters.\n",
    "$$P(X_\\text{new}|X_\\text{old})=P(X_\\text{new}|\\theta)$$\n",
    "where $$\\theta = \\text{argmax}_\\theta P(X_\\text{old}|\\theta)$$\n",
    "We use $D$ to refer to old data in some cases. <br>\n",
    "Often it is assumed that the data is independent and identically distributed, which means:\n",
    "$$P(D|\\theta)=\\prod_i P(D_i|\\theta)$$\n",
    "Another common practice is to use the log likelihood of the data, as $$\\text{argmax} (x) = \\text{argmax} (\\log(x))$$\n",
    "This turns the above product into a sum:\n",
    "$$P(D|\\theta)\\propto \\sum_i \\log(P(D_i|\\theta))$$\n",
    "This is much more numerically stable as a product of many numbers less than 1 gets very very small. <br>\n",
    "**examples of maximum likelihood estimates:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e5e23f",
   "metadata": {},
   "source": [
    "### Categorical\n",
    "The binomial distribution describes the distribution with only two possible outcomes defined by a single value parameter $\\theta$. Samples are akin to flipping a coin with a certain bentness. The predicted value $k$ is the number of times one outcome occurs in a given number of samples $n$. The number of times the other result happens is just $n-k$. The probability is defined for $\\theta$: <br>\n",
    "$$P(k|n,\\theta)=\\frac{\\theta^k (1-\\theta)^{n-k}n!}{k!(n-k)!}$$\n",
    "We want a distribution over $\\theta$ and are using the maximum likelihood to do this so need to take the $\\text{argmax}(\\theta)$ of the above. Constants and normalizations not depending on $\\theta$ can thus be removed: \n",
    "$$P(k|n,\\theta)\\propto \\theta^k (1-\\theta)^{n-k}$$\n",
    "Maximizing this is the same as maximizing the likelihood, so:\n",
    "$$l(\\theta)\\propto k\\ln(\\theta)+(n-k)\\ln(1-\\theta)$$\n",
    "As the function is convex we can set the gradient to 0 to get the maximum:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "  \\nabla l(\\theta)&=\\frac{k}{\\theta}-\\frac{n-k}{1-\\theta} \\\\\n",
    "  0&=\\frac{k}{\\theta}-\\frac{n-k}{1-\\theta} \\\\\n",
    "  \\frac{n-k}{1-\\theta}&=\\frac{k}{\\theta} \\\\\n",
    "  \\frac{(n-k)\\theta}{(1-\\theta)\\theta}&=\\frac{k(1-\\theta)}{(1-\\theta)\\theta} \\\\\n",
    "  (n-k)\\theta&=k(1-\\theta) \\\\\n",
    "  n\\theta-k\\theta&=k-k\\theta \\\\\n",
    "  n\\theta&=k \\\\\n",
    "  \\theta&=\\frac{k}{n}\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1462bfc3",
   "metadata": {},
   "source": [
    "So the best estimate for the \"bentness\" is just the mean. With enough data this approaches the truth. E.g For a coin:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e314d77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta estimate 0.6\n"
     ]
    }
   ],
   "source": [
    "total_heads = 0\n",
    "total_flips = 0\n",
    "theta = 0.6\n",
    "for sample in range(10000):\n",
    "    if(np.random.rand()<theta):\n",
    "        total_heads+=1\n",
    "    total_flips+=1\n",
    "prob_heads_estimates = total_heads/total_flips\n",
    "print(\"theta estimate\",prob_heads_estimates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd092af",
   "metadata": {},
   "source": [
    "The same formula also applies for $j$ discrete variables. The maximum likelihood is:\n",
    "$$\\theta_j=\\frac{k_j}{\\sum k}$$\n",
    "Example of a fair dice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e4b328f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "truth    [0.1667 0.1667 0.1667 0.1667 0.1667 0.1667]\n",
      "estimate [0.1663 0.1696 0.1672 0.1623 0.1726 0.162 ]\n"
     ]
    }
   ],
   "source": [
    "totals = np.zeros(6)\n",
    "dice_true_probs = np.ones(6)*(1/6)\n",
    "for sample in range(10000):\n",
    "    roll = np.random.choice(np.arange(1,7),p=dice_true_probs)\n",
    "    totals[roll-1]+=1\n",
    "estimated_probs = totals/np.sum(totals)\n",
    "print(\"truth   \",dice_true_probs.round(4))\n",
    "print(\"estimate\",estimated_probs.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3571830a",
   "metadata": {},
   "source": [
    "### Gaussian\n",
    "The gaussian pdf is:\n",
    "$$ p(x|\\mu,\\sigma^2)=\\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-\\frac{1}{2} \\frac{(x-\\mu)^2}{\\sigma^2}}$$\n",
    "The log is:\n",
    "$$ \\begin{aligned}\n",
    "    p(x|\\mu,\\sigma^2)&=\\log(\\frac{1}{\\sigma\\sqrt{2\\pi}})-\\frac{1}{2} \\frac{(x-\\mu)^2}{\\sigma^2} \\\\\n",
    "    &=-\\log(\\sigma\\sqrt{2\\pi})-\\frac{1}{2} \\frac{(x-\\mu)^2}{\\sigma^2} \\\\\n",
    "    &=-\\log(\\sigma) -\\log(\\sqrt{2\\pi})-\\frac{1}{2} \\frac{(x-\\mu)^2}{\\sigma^2}\n",
    "\\end{aligned}\n",
    "$$\n",
    "So for $n$ iid (independent identically distributed) data points this becomes the log likelihood:\n",
    "$$-n\\log(\\sigma) -n\\log(\\sqrt{2\\pi})-\\sum_{i=1}^n \\frac{1}{2} \\frac{(x_i-\\mu)^2}{\\sigma^2}$$\n",
    "Which is: \n",
    "$$-n\\log(\\sigma) -n\\log(\\sqrt{2\\pi})- \\frac{1}{2} \\frac{\\sum_{i=1}^n(x_i-\\mu)^2}{\\sigma^2}$$\n",
    "The constant $-n\\log(\\sqrt{2\\pi})$ can be dropped when doing MLE <br>\n",
    "This function is also convex, so we can set the gradient to 0 for each variable.\n",
    "$$ \\begin{aligned}\n",
    "    \\nabla\\mu&=\\frac{\\sum_{i=1}^n(x_i-\\mu)}{\\sigma^2} \\\\\n",
    "    0&=\\frac{\\sum_{i=1}^n(x_i-\\mu)}{\\sigma^2} \\\\\n",
    "    0&=\\sum_{i=1}^n(x_i-\\mu) \\\\\n",
    "    \\sum_{i=1}^n \\mu&=\\sum_{i=1}^n x_i \\\\\n",
    "    n\\mu&=\\sum_{i=1}^n x_i \\\\\n",
    "    \\mu&=\\frac{1}{n}\\sum_{i=1}^n x_i \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "So $\\mu$ is just the mean of the samples\n",
    "$$ \\begin{aligned}\n",
    "    \\nabla\\sigma^2&=-\\frac{n}{\\sigma}+\\frac{\\sum_{i=1}^n(x_i-\\mu)^2}{\\sigma^3} \\\\\n",
    "    0&=-\\frac{n}{\\sigma}+\\frac{\\sum_{i=1}^n(x_i-\\mu)^2}{\\sigma^3} \\\\\n",
    "    \\frac{n}{\\sigma}&=\\frac{\\sum_{i=1}^n(x_i-\\mu)^2}{\\sigma^3} \\\\\n",
    "    n&=\\frac{\\sum_{i=1}^n(x_i-\\mu)^2}{\\sigma^2} \\\\\n",
    "    \\sigma^2&=\\frac{\\sum_{i=1}^n(x_i-\\mu)^2}{n}\n",
    "\\end{aligned}\n",
    "$$\n",
    "So, $\\sigma^2$ is just the variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89e6d312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true mean      3.4053 true sigma      1.4233\n",
      "estimated mean 3.418 estimated sigma 1.4077\n"
     ]
    }
   ],
   "source": [
    "true_mean = 3.4053\n",
    "true_sigma = 1.4233\n",
    "gaussian_samples = np.random.normal(true_mean,true_sigma,10000)\n",
    "estimated_mean = np.mean(gaussian_samples)\n",
    "estimated_sigma = np.sqrt(np.mean((gaussian_samples-estimated_mean)**2))\n",
    "print(\"true mean     \",true_mean,\"true sigma     \",true_sigma)\n",
    "print(\"estimated mean\",estimated_mean.round(4),\"estimated sigma\",estimated_sigma.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24c1728",
   "metadata": {},
   "source": [
    "### Bayesian Networks\n",
    "Say we have variables $X_{1:n}$ and a a set of $m$ data points $D$ for a Bayesian Network. A single point $d$ in $D$ is defined as the values for each variable $X_{1:n}$. The function $\\text{par}(d,i)$ gets the parents for variable $i$ instantiated in $d$. Each variable has $r_i$ different possible values. We refer to the parents of a given node $i$ for a given instantiation $j$ as $\\pi_{ij}$. We refer to the number of parent configurations as $q_i$. The probability of a single variable being set to $k$ is given by the factor:\n",
    "$$p(X_i=k|\\pi_{ij})=\\theta_{ijk}$$\n",
    "For $d$ the probability of a single variable is the value $\\theta_{ijk}$ where $d_i=k$ and $\\text{par}(d,i)=\\pi_{ij}$. If we have a counting function $\\mathcal{1}$ which is 1 where true this can be expressed: \n",
    "$$p(d_i)=\\prod_{k=1}^{r_i}\\prod_{j=1}^{q_i}\\theta_{ijk}\\mathcal{1}(d_i=k)\\mathcal{1}(\\text{par}(d,i)=\\pi_{ij})$$\n",
    "As each variable is independent given the parents, the probability for every variable is:\n",
    "$$p(d)=\\prod_{i=1}^n\\prod_{k=1}^{r_i}\\prod_{j=1}^{q_i}\\theta_{ijk}\\mathcal{1}(d_i=k)\\mathcal{1}(\\text{par}(d,i)=\\pi_{ij})$$\n",
    "As each data point is independent the likelihood can be expressed:\n",
    "$$l(D)=\\prod_{u=1}^m\\prod_{i=1}^n\\prod_{k=1}^{r_i}\\prod_{j=1}^{q_i}\\theta_{ijk}\\mathcal{1}(D_{ui}=k)\\mathcal{1}(\\text{par}(D_u,i)=\\pi_{ij})$$\n",
    "The product over all data items can be replaced by a counting function $M$:\n",
    "$$l(D)=\\prod_{i=1}^n\\prod_{k=1}^{r_i}\\prod_{j=1}^{q_i}\\theta_{ijk}^{M_{ijk}}$$\n",
    "Where $M_{ijk}$ is the number of times variable $i$ is set to $k$ with parents $j$ across the data.<br>\n",
    "Similarly to the categorical distribution the maximum likelihood in the network is:\n",
    "$$\\theta=\\frac{M_{ijk}}{\\sum_k M_{ijk}}$$\n",
    "**Example:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d07744f",
   "metadata": {},
   "outputs": [],
   "source": [
    "factorA = factors.Factor([\"A\"],[2])\n",
    "factorB = factors.Factor([\"B\"],[2])\n",
    "factorC_givenAB = factors.Factor([\"C\",\"A\",\"B\"],[2,2,2])\n",
    "factorD_givenC = factors.Factor([\"D\",\"C\"],[2,2])\n",
    "factorE_givenC = factors.Factor([\"E\",\"C\"],[2,2])\n",
    "factorA.set_all([0.7,0.3])\n",
    "factorB.set_all([0.75,0.25])\n",
    "factorC_givenAB.set_all([0.05,0.5,0.7,0.45,0.95,0.5,0.3,0.55])\n",
    "factorD_givenC.set_all([0.2,0.7,0.8,0.3])\n",
    "factorE_givenC.set_all([0.6,0.15,0.4,0.85])\n",
    "all_true_factors = [factorA,factorB,factorC_givenAB,factorD_givenC,factorE_givenC]\n",
    "samples = []\n",
    "for s in range(1000):\n",
    "    sample_variable_names,sample = factors_sampling.joint_sample_top_down(all_true_factors)\n",
    "    samples.append(sample)\n",
    "samples = np.array(samples)\n",
    "empty_factors = [f.copy_zeros() for f in all_true_factors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68856d47",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def MLE_directed_bayes_net(all_factors,sample_variable_names,samples):\n",
    "    total_prob = 1\n",
    "    new_factors = [f.copy() for f in all_factors]\n",
    "    for j in range(len(all_factors)):\n",
    "        indexes = all_factors[j].indexes\n",
    "        factor_to_sample_index = [sample_variable_names.index(name) for name in all_factors[j].names]\n",
    "        selected_samples = samples[:,factor_to_sample_index]\n",
    "        counts = np.zeros(indexes.shape[0])\n",
    "        for i in range(indexes.shape[0]):\n",
    "            match = (selected_samples==indexes[i])\n",
    "            all_match = np.prod(match,axis=1)\n",
    "            counts[i] = np.sum(all_match)\n",
    "            new_factors[j].set(indexes[i],counts[i])\n",
    "        new_factors[j]=factors.condition(new_factors[j],axis=new_factors[j].names[1:])\n",
    "    return new_factors\n",
    "learned_factors = MLE_directed_bayes_net(empty_factors,sample_variable_names,samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba772828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRUE\n",
      "A  Values (10 dp)\n",
      "0  0.7\n",
      "1  0.3\n",
      "\n",
      "LEARNED\n",
      "A  Values (10 dp)\n",
      "0  0.741\n",
      "1  0.259\n",
      "\n",
      "TRUE\n",
      "B  Values (10 dp)\n",
      "0  0.75\n",
      "1  0.25\n",
      "\n",
      "LEARNED\n",
      "B  Values (10 dp)\n",
      "0  0.733\n",
      "1  0.267\n",
      "\n",
      "TRUE\n",
      "C  A  B  Values (10 dp)\n",
      "0  0  0  0.05\n",
      "0  0  1  0.5\n",
      "0  1  0  0.7\n",
      "0  1  1  0.45\n",
      "1  0  0  0.95\n",
      "1  0  1  0.5\n",
      "1  1  0  0.3\n",
      "1  1  1  0.55\n",
      "\n",
      "LEARNED\n",
      "C  A  B  Values (10 dp)\n",
      "0  0  0  0.0560747664\n",
      "0  0  1  0.4854368932\n",
      "0  1  0  0.7070707071\n",
      "0  1  1  0.4426229508\n",
      "1  0  0  0.9439252336\n",
      "1  0  1  0.5145631068\n",
      "1  1  0  0.2929292929\n",
      "1  1  1  0.5573770492\n",
      "\n",
      "TRUE\n",
      "D  C  Values (10 dp)\n",
      "0  0  0.2\n",
      "0  1  0.7\n",
      "1  0  0.8\n",
      "1  1  0.3\n",
      "\n",
      "LEARNED\n",
      "D  C  Values (10 dp)\n",
      "0  0  0.202020202\n",
      "0  1  0.6870554765\n",
      "1  0  0.797979798\n",
      "1  1  0.3129445235\n",
      "\n",
      "TRUE\n",
      "E  C  Values (10 dp)\n",
      "0  0  0.6\n",
      "0  1  0.15\n",
      "1  0  0.4\n",
      "1  1  0.85\n",
      "\n",
      "LEARNED\n",
      "E  C  Values (10 dp)\n",
      "0  0  0.5824915825\n",
      "0  1  0.146514936\n",
      "1  0  0.4175084175\n",
      "1  1  0.853485064\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for a in range(len(all_true_factors)):\n",
    "    print(\"TRUE\")\n",
    "    print(all_true_factors[a])\n",
    "    print(\"LEARNED\")\n",
    "    print(learned_factors[a])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
