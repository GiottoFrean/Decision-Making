{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In probabilistic machine learning we frame problems in terms of some $X$, $D$ and $\\theta$, where $X$ is an unknown data/event we want to model, $D$ is our seen data, and $\\theta$ represents the parameters of our model. We tend to want to know $p(X|D)$, or $p(X|\\theta)$, where $\\theta$ is learned from $D$. <br> \n",
    "Some models don't include a $\\theta$. These are called non-parametric models, and often don't scale well with data.\n",
    "In a parametric model knowledge from the known data ($D$) is encoded into the parameters $\\theta$. This means $p(X|\\theta,D)=p(X|\\theta)$ <br>\n",
    "In general there are three options for doing predictions in this setting: <br>\n",
    "1. Max Likelihood Esimation (MLE)\n",
    "2. Maximum a posteriori (MAP)\n",
    "3. Sum/Integrate out $\\theta$\n",
    "\n",
    "*Option 1:* We say $p(X|D)$ is essentially the same as $p(X|\\theta)$, where $\\theta = \\text{argmax}_\\theta p(D|\\theta)$ Essentially we maximize the probability of the known data under the model parameters, then use those parameters to predict our unknown data. <br> <br>\n",
    "*Option 2:* Similarly, $p(X|D)$ is modeled $p(X|\\theta)$, but $\\theta = \\text{argmax}_\\theta p(\\theta|D)$. This is a subtle change, in that now we find the most likely values for $\\theta$, given the data, which involves a prior: $ \\text{argmax}_\\theta p(\\theta|D) = \\text{argmax}_\\theta p(D|\\theta)p(\\theta)$ (using bayes rule, ignoring $p(D)$ denominator, as it is a constant and dropped when finding the maximum)<br> <br>\n",
    "*Option 3:* We expand $p(X|D) = \\sum_{\\theta} p(X,\\theta|D) = \\sum_{\\theta} p(X|\\theta,D)p(\\theta|D) $ <br>\n",
    "Which can be further simplified: $= \\sum_{\\theta} p(X|\\theta)p(\\theta|D)$, as we are using a parametric model. We multiply the probability of $X$ conditioned on $\\theta$ by the probability of $\\theta$ conditioned on $D$, then integrate out $\\theta$. This is the most powerful way to learn $p(X|D)$, but relies on finding $p(D)$ which is often intractable. <br> <br>\n",
    "To show these different approaches, consider the following example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bent coin example: <br>\n",
    "Consider that I have a coin which has a bentness, $\\theta$. This bentness gives the probability of getting heads on any throw. Say there are only 3 options for $\\theta$: A fair coin ($\\theta=0.5$), heads on both sides ($\\theta=1$) and tails on both sides ($\\theta=0$). My coin is one of those three options. <br>\n",
    "Say I flip the coin once and show the results to give $D$, the known data. I will then flip it again later to get $X$, the unknown data. If $\\theta$ is known trials are independent, so $p(D|\\theta)$ and $p(X|\\theta)$ are known: <br>\n",
    "\n",
    "|         | H    |  T   |\n",
    "|---------|------|------|\n",
    "| **0**   | 0    | 1    |\n",
    "| **0.5** | 0.5  | 0.5  |\n",
    "| **1**   | 1    | 0    |\n",
    "\n",
    "Say I observe D=H, and want to know $p(X=H|D=H)$.\n",
    "\n",
    "**1: Max likelihood**\n",
    "\n",
    "The value of $\\theta$ with the highest $p(D|\\theta)$ is $\\theta=1$. Therefore, to predict the probability $p(X=H|D=H)$, I get $p(X=H|\\theta=1)=1$\n",
    "\n",
    "**2: Maximum a posteriori**\n",
    "\n",
    "Say my prior $p(\\theta)$ is $p(\\theta=0)=0.1$,  $p(\\theta=0.5)=0.8$,  $p(\\theta=1)=0.1$. Then: <br>\n",
    "$p(\\theta=0|D=H) \\propto 0\\times 0.1 $ <br>\n",
    "$p(\\theta=0.5|D=H) \\propto 0.5 \\times 0.8 $ <br>\n",
    "$p(\\theta=1|D=H) \\propto 1\\times 0.1 $ <br>\n",
    "\n",
    "These values are $0$, $0.4$ and $0.1$ respectively. So the most likely MAP value of $\\theta$ is $0.5$.\n",
    "\n",
    "Therefore, to predict the probability $p(X=H|D=H)$, I get $p(X=H|\\theta=0.5)=0.5$\n",
    "\n",
    "**3: Sum out $\\theta$**\n",
    "\n",
    "We want $\\sum_{\\theta} p(X=H|\\theta)p(\\theta|D=H)$. We know from the MAP values $p(\\theta|D=H)$, unnormalized. Using bayes rule $p(\\theta|D)$ is actually $p(D|\\theta)p(\\theta)\\div p(D)$. $p(D)$ is constant, so is left out when finding the max likelihood and MAP estimates. <br>\n",
    "We know the proportional values of $p(\\theta|D=H)$ sum as $0+0.4+0.1 = 0.5$, so have $p(D)=0.5$ <br> \n",
    "Therefore: <br>\n",
    "$p(\\theta=0|D=H)=0 \\div 0.5 = 0$ <br>\n",
    "$p(\\theta=0.5|D=H)=0.4 \\div 0.5 = 0.8 $ <br>\n",
    "$p(\\theta=1|D=H)=0.1 \\div 0.5 = 0.2 $ <br>\n",
    "So: <br>\n",
    "$\\sum_{\\theta} p(X=H|\\theta)p(\\theta|D=H) = 0\\times 0 + 0.8 \\times 0.5 + 0.2 \\times 1 = 0.6 $ <br>\n",
    "\n",
    "This third option makes the most sense, but relies on calculating the normalized probabilities of $\\theta$ given the data, which is intractable in many cases. One case where it is not is the beta-binomial model. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Very simple way of testing these models:** <br>\n",
    "For testing just assume the prior is correct, then throw away all samples which don't generate $D=H$. Then generate another sample, $X$, and count how many times $X=H$. This is monte-carlo sampling, which is covered later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "esimated p(X=H|D=H) = 0.594 %\n"
     ]
    }
   ],
   "source": [
    "X_H_counts = 0\n",
    "X_total_counts = 0\n",
    "for trial in range(10000):\n",
    "    coin = np.random.choice([0,0.5,1],p=[0.1,0.8,0.1])\n",
    "    D = np.random.choice([\"H\",\"T\"],p=[coin,1-coin])\n",
    "    if(D==\"H\"):\n",
    "        X = np.random.choice([\"H\",\"T\"],p=[coin,1-coin])\n",
    "        if(X==\"H\"):\n",
    "            X_H_counts+=1\n",
    "        X_total_counts+=1\n",
    "\n",
    "print(\"esimated p(X=H|D=H) =\", np.round(X_H_counts/X_total_counts,3), \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the ratio is almost exactly 0.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extension of the bent coin example:\n",
    "Now considering a coin which can have any bentness in the range 0 to 1. $\\theta$ is still the probability of getting heads. <br>\n",
    "$p(D=H|\\theta)=\\theta$ <br>\n",
    "$p(D=T|\\theta)=1-\\theta$ <br>\n",
    "Say we have a uniform prior: <br>\n",
    "$p(\\theta)=1$ in the range $[0-1]$ (the integration of $p(\\theta)$ is 1, so this is a valid distribution) <br>\n",
    "So, again saying we want to know $p(X=H|D=H)$: <br>\n",
    "\n",
    "**1: MLE** <br>\n",
    "$p(X=H|\\theta)=\\theta.$ <br>\n",
    "$\\text{argmax}_\\theta p(D=H|\\theta) = \\text{argmax}_\\theta \\theta = 1$ <br>\n",
    "Therefore: $p(X=H|\\theta)=1$ <br> <br>\n",
    "\n",
    "**2: MAP** <br>\n",
    "$\\text{argmax}_\\theta p(\\theta|D=H) = \\text{argmax}_\\theta \\theta\\times1 = 1$ <br>\n",
    "Therefore: $p(X=H|\\theta)=1$ <br> <br>\n",
    "\n",
    "**3: Integrate out $\\theta$** <br>\n",
    "$\\int_\\theta p(X=H|\\theta)p(\\theta|D=H) = \\theta \\times 1 \\times \\theta \\div p(D=H)$ <br>\n",
    "So, now to find $p(D=H)$. Luckily, we can do that in this case: <br>\n",
    "$\\int_\\theta p(\\theta|D=H)=1$ <br>\n",
    "So, the integral of the posterior must be 1. The full posterior using bayes rule is: $p(D=H|\\theta)p(\\theta)\\div p(D=H)$. <br>\n",
    "As $p(D=H)$ does not include $\\theta$, it can be moved out of the integral over $\\theta$:<br>\n",
    "$1 = (\\int_\\theta p(D=H|\\theta)p(\\theta)) \\div p(D=H)$ <br>\n",
    "$1 = (\\int_\\theta \\theta \\ times 1) \\div p(D=H)$ <br>\n",
    "The integration of $\\theta$ between 0 and 1 $=\\left[\\frac{1}{2}\\theta\\right]_0^1 = 0.5$ <br>\n",
    "So: $1 = 0.5 \\div p(D=H)$. Therefore, $p(D)=0.5$<br>\n",
    "Then: <br>\n",
    "$\\int_\\theta p(X=H|\\theta)p(\\theta|D=H) = \\theta \\times 1 \\times \\theta \\div 0.5 $ <br>\n",
    "$= \\int_\\theta {2 \\theta^2 }$ <br>\n",
    "$= [2 \\div 3 \\times \\theta^3]_0^1$ <br>\n",
    "$= \\frac{2}{3}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another simple test, same as before (but with the uniform prior instead):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "esimated p(X=H|D=H) = 0.659 %\n"
     ]
    }
   ],
   "source": [
    "X_H_counts = 0\n",
    "X_total_counts = 0\n",
    "for trial in range(10000):\n",
    "    coin = np.random.rand() # the uniform prior\n",
    "    D = np.random.choice([\"H\",\"T\"],p=[coin,1-coin])\n",
    "    if(D==\"H\"):\n",
    "        X = np.random.choice([\"H\",\"T\"],p=[coin,1-coin])\n",
    "        if(X==\"H\"):\n",
    "            X_H_counts+=1\n",
    "        X_total_counts+=1\n",
    "print(\"esimated p(X=H|D=H) =\", np.round(X_H_counts/X_total_counts,3), \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As predicted the estimated probability is around 2/3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Beta-Binomial model\n",
    "Now the model is extended to talk about any amount of data $D$ (not just heads). As all coin flip data is iid (independent, identically distributed) we still only need to care about $p(X=H|D)$ and $p(X=T|D)$. So the only thing that changes is the likelihood and prior. <br>\n",
    "For $n$ tosses of the coin, $h$ heads and $n-h$ tails, then the likelihood is: <br>\n",
    "$p(D|\\theta)=\\theta^h(1-\\theta)^{n-h}$ <br>\n",
    "However, the above assumes we know the order of the flips of the coins. HHT and HTH should give the same final value for $\\theta$ as the data is iid. If we say our data is only the numbers $h$ and $n$ (without the exact order), then we have the binomial distribution: <br>\n",
    "$p(D|\\theta)=\\left(\\frac{n}{h}\\right)\\theta^h(1-\\theta)^{n-h}$ <br>\n",
    "We also need a more informative prior than just a uniform distribution, and one which is defined over the range 0 to 1. <br>\n",
    "We want a prior that looks similar to the likelihood to make the maths easy. The beta distribution is that prior: <br>\n",
    "$\\text{Beta}(\\theta|a,b)\\propto \\theta^{a-1}(1-\\theta)^{b-1}$ <br>\n",
    "$a$ and $b$ are hyper-parameters. With a value of $a=1$ and $b=1$ then the distribution is the uniform one.\n",
    "Therefore: \n",
    "$p(D|\\theta)p(\\theta) \\propto \\left(\\frac{n}{h}\\right)\\theta^h(1-\\theta)^{n-h} \\theta^{a-1}(1-\\theta)^{b-1}$ <br>\n",
    "$p(D|\\theta)p(\\theta) \\propto \\left(\\frac{n}{h}\\right)\\theta^{h+a-1}(1-\\theta)^{n-h+b-1}$ <br>\n",
    "We can recognise the above equation as proportional to the beta distribution again. This makes the beta distribution a conjugate prior. <br>\n",
    "The normalization of the beta distribution is known for any $a$ and $b$, so plugging in $h+a$ and $n-h+b$ gives the full posterior (normalized): <br>\n",
    "\n",
    "$p(\\theta|D) = \\text{Beta}(h+a,n-h+b)$ <br>\n",
    "So, now it is possible to integrate out $\\theta$: <br>\n",
    "$\\int_\\theta p(X=H|\\theta)p(\\theta|D) = \\int_\\theta \\theta \\text{Beta}(h+a,n-h+b)$ <br>\n",
    "This ends up simply being: <br>\n",
    "$ (h+a) \\div (h+a + n-h+b)$ <br>\n",
    "$ = (h+a) \\div (a+n+b)$ <br>\n",
    "**Demonstrating:** <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prob for a single toss after seeing heads 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "def beta_bin_model_prob_heads(a,b,n,h):\n",
    "    return (h+a)/(a+n+b)\n",
    "print(\"prob for a single toss after seeing heads\",beta_bin_model_prob_heads(1,1,1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The answer is exactly correct, in line with the predictions for the D=H case. <br>\n",
    "Checking with Monte-Carlo sampling (for any a,b,n,h):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta-binomial p(X=H|D=H) =  0.3333333333333333\n",
      "esimated p(X=H|D=H) = 0.333 %\n"
     ]
    }
   ],
   "source": [
    "a = 1\n",
    "b = 1\n",
    "n = 16\n",
    "h = 5\n",
    "\n",
    "X_H_counts = 0\n",
    "X_total_counts = 0\n",
    "for trial in range(10000):\n",
    "    coin = np.random.beta(a,b)\n",
    "    D = np.random.choice([\"H\",\"T\"],n,p=[coin,1-coin])\n",
    "    heads = len(D[D==\"H\"])\n",
    "    if(heads==h):\n",
    "        X = np.random.choice([\"H\",\"T\"],p=[coin,1-coin])\n",
    "        if(X==\"H\"):\n",
    "            X_H_counts+=1\n",
    "        X_total_counts+=1\n",
    "\n",
    "print(\"beta-binomial p(X=H|D=H) = \", beta_bin_model_prob_heads(a,b,n,h))\n",
    "print(\"esimated p(X=H|D=H) =\", np.round(X_H_counts/X_total_counts,3), \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predictions of the beta-binomial model are largely in line with the results from sampling. Though, obviously with a high value of n then most coins will just be thrown out. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
